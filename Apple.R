library(timeDate)
library(timeSeries)
library(fBasics)
library(quantmod)

stockname = 'AAPL'
startdate = '2016-01-01'
enddate = '2019-01-01'

stockvar <- getSymbols(c(stockname), src = 'yahoo', from = startdate, to = enddate, 
                       auto.assign = FALSE)

stockvar = na.omit(stockvar)

price = stockvar[, 4]
price
plot(price)

#logsouvenirtimeseries <- log(datatimeseries)
#plot.ts(logsouvenirtimeseries)

#Decomposing the timeseries data
library(TTR)

# get the estimated values of the seasonal component
datatimeseries <- ts(price, frequency = 120, start=c(2016-01-01))
datatimeseries
head(datatimeseries)
plot(datatimeseries)

AAPLtimeseriescomponents <- decompose(datatimeseries)
AAPLtimeseriescomponents$seasonal
plot(AAPLtimeseriescomponents)

#Seasonally Adjusting
AAPLtsseasonallyadjusted <- datatimeseries - AAPLtimeseriescomponents$seasonal
#Plotting the seasonally adjusted time series
plot(AAPLtsseasonallyadjusted) #The seasonally adjusted time series now just contains the trend component and an irregular component
#We can see there has only been a slight change by removing the seasonality

#Smooth data
SMA_timeseries <- SMA(datatimeseries, n=25)
plot(SMA_timeseries)

#ADF testing

library(forecast)
plot(datatimeseries, type="l") # original series
plot(AAPLtsseasonallyadjusted, type="l")  # seasonally adjusted
seasonplot(AAPLtsseasonallyadjusted, 120, col=rainbow(12), year.labels=TRUE, main="Seasonal plot: AAPL") # seasonal frequency set as 12 for monthly data.


#Getting Auto & Partial Correlation
acfRes <- acf(price) # autocorrelation
#Autocorrelation is the correlation of a Time Series with lags of itself
pacfRes <- pacf(price)  # partial autocorrelation
pacfRes
#Partial Autocorrelation is the correlation of the time series with a lag of itself, with the linear dependence of all the lags between them removed.

trModel <- lm(datatimeseries ~ c(1:length(datatimeseries)))
plot(resid(trModel), type="l")  # resid(trModel) contains the de-trended series

#################################################################################
 
                           #GRAMEVOL FUNCTION#

#################################################################################

library(gramEvol)

#Scaling data
#scale(datatimeseries)

# Define a window size - let's say 3
new_price <- data.frame(x3=Lag(price,3), x2=Lag(price,2),
                                x1=Lag(price,1), price)
new_price
names(new_price) <- c('x3','x2','x1','x')
head(new_price)

#Create training & test data
train <- new_price[4: 600, ]
train
test <- new_price[601:754, ]
test

#define our grammar
newRules <- list(expr = grule(op(expr, expr), func(expr), var),
                 func = grule(sin, cos, exp),
                 op = grule('+', '-', '*', '/', '^'),
                 var = grule(mydata$x3, mydata$x2, mydata$x1))

# Then need to create grammar from rules
newGram <- CreateGrammar(newRules)
newGram

# Fitness function (RMSE)
newFitFunc <- function(expr) {
  result <- eval(expr)
  if (any(is.nan(result)))
    return(Inf)
  return (sqrt(mean((mydata$x - result)^2)))
  #return(mean(log(1 + abs(mydata$x - result))))
}

#Now use the training data to evolve a function to predict values
mydata <- train

ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.01, 
                           iterations = 2500, max.depth = 5)
ge

#Grammatical Evolution Search Results:
#No. Generations:  2500 
#Best Expression:  mydata$x1/cos(sin(mydata$x3)/mydata$x1) 
#Best Cost:        1.85490802824918 

# Get the result out
ge$best$expressions

# Look at the values generated by training
eval(ge$best$expressions)
best_train <- eval(ge$best$expressions)
best_train
eval_train <- data.frame(mydata[1:5, 1], best_train[1:5])
eval_train

#ACCURACY SCORE#
accuracy(mydata[1:5, 1], best_train[1:5])

#Then look at the performance over the test data
mydata <- test
eval(ge$best$expressions)
best_test <- eval(ge$best$expressions)
best_test
eval_test <- data.frame(mydata[1:5, 1], best_test[1:5])
eval_test

#ACCURACY SCORE#
accuracy(mydata[1:5, 1], best_test[1:5])

## Predicting more than one step ahead ##
# Now use this to make further predictions - i.e. use generated values rather than exiting ones

# grab the last 3 (or whaever your window size is) values from the training data
mydata <- train[597, 1:3]
mydata

# initiaise an array of predictions
predictions <-c()

# let's try and predict 5 values ahead, each time feeding in 
# the predicted value to use as input for new predictions

for(i in 1:5){
  predictions[i] <- eval(ge$best$expressions)
  # shufle data along and insert new prediction
  # put this in a loop for a large window size
  mydata[1] <- mydata[2] 
  mydata[2] <- mydata[3]
  mydata[3] <- predictions[i]}

# See what the predictions are
predictions

test_results <- data.frame(test[1:5, 1], predictions)
test_results

#ACCURACY SCORE#
accuracy(test[1:5, 1], predictions)

#Based of the RMSE score, we can see that our function, with a lag of 3, made fairly accurate predictions

#################################################################################

                  #FORECASTING USING OTHER METHODS#

#################################################################################

#We can utilise other forecasting methods to see if they can perform better than our GramEvol model.

#Holts Exponential Smoothing
#This method works with data that has no seasonality, or has been seasonally adjusted.

library(forecast)

HES_train <- AAPLtsseasonallyadjusted[1: 600, ]
HES_test <- AAPLtsseasonallyadjusted[601: 754, ]

AAPLseriesforecasts <- HoltWinters(HES_train, gamma=FALSE)
AAPLseriesforecasts
AAPLseriesforecasts$SSE #Sum of squared erros value
plot(AAPLseriesforecasts)
#We can see from the picture that the in-sample forecasts agree pretty well with the observed values, although they tend to lag behind the observed values a little bit.

#specifying the initial values of the level and the slope b of the trend component
#level = first value in time series data
#slope value = second value - first value 

head(HES_train)
HoltWinters(price, gamma=FALSE, l.start=105.14264, b.start=-0.63739)

#Making forecasts

AAPLseriesforecasts2 <- forecast:::forecast.HoltWinters(AAPLseriesforecasts, h=252)
AAPLseriesforecasts2
#comparison, load in 2020 data
plot(AAPLseriesforecasts2)

#The forecasts are shown as a blue line 
#The 80% prediction intervals as a light blue shaded area 
#The 95% prediction intervals as a light grey shaded area

#Checking whether the predictive model could be improved upon by checking whether the in-sample forecast errors show non-zero autocorrelations at lags 1-20.

AAPLseriesforecasts2_residuals <- AAPLseriesforecasts2$residuals
#na.omit(AAPLseriesforecasts2$residuals)

acf(AAPLseriesforecasts2$residuals, lag.max=20, na.action = na.pass)
Box.test(AAPLseriesforecasts2$residuals, lag=20, type="Ljung-Box")

#Here the correlogram shows that the sample autocorrelation for the in-sample forecast errors at lag 8 exceeds the significance bounds.
#the p-value is 0.0409, indicating that there is little evidence of non-zero autocorrelations in the in-sample forecast errors at lags 1-20.

#Checking that the forecast errors have constant variance over time.
#And are normally distributed with mean zero.

plot.ts(AAPLseriesforecasts2$residuals) #make a time plot
#The time plot shows that the forecast errors have roughly constant variance over time

#Replace null values with mean
#for (i in 1:ncol(AAPLseriesforecasts2$residuals)){
 # AAPLseriesforecasts2$residuals[is.na(AAPLseriesforecasts2$residuals[,i]), i] <- mean(AAPLseriesforecasts2$residuals[,i], na.rm = TRUE)
#}

plotForecastErrors <- function(forecasterrors)
{
  # make a histogram of the forecast errors:
  mybinsize <- IQR(forecasterrors)/4
  mysd   <- sd(forecasterrors)
  mymin  <- min(forecasterrors) - mysd*5
  mymax  <- max(forecasterrors) + mysd*3
  # generate normally distributed data with mean 0 and standard deviation mysd
  mynorm <- rnorm(10000, mean=0, sd=mysd)
  mymin2 <- min(mynorm)
  mymax2 <- max(mynorm)
  if (mymin2 < mymin) { mymin <- mymin2 }
  if (mymax2 > mymax) { mymax <- mymax2 }
  # make a red histogram of the forecast errors, with the normally distributed data overlaid:
  mybins <- seq(mymin, mymax, mybinsize)
  hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
  # freq=FALSE ensures the area under the histogram = 1
  # generate normally distributed data with mean 0 and standard deviation mysd
  myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
  # plot the normal curve as a blue line on top of the histogram of forecast errors:
  points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
#Starting at 3 as frrst 2 values were null and the for-loop above wasnt working
plotForecastErrors(AAPLseriesforecasts2_residuals[3:600]) #make a histogram
#The histogram highlights that for the most part there is constant variance within the forecast erros


#Holt Winters Exponential Smoothing

#logdatatimeseries <- log(train) #Log of timeseries with seasonality
#HW_train = datatimeseries[1:600]
#AAPLseriesforecasts_HW <- HoltWinters(HW_train)
#AAPLseriesforecasts_HW

#Smoothing parameters:
#alpha: 0.8861502, this is relatively high, meaning that the estimates are based upon mostly recent observtions
#beta : 0, this value indicates that hat the estimate of the slope b of the trend component is not updated over the time series, and instead is set equal to its initial value
#gamma: 1, the value of gamma is high, indicating that the estimate of the seasonal component at the current time point is just based upon very recent observations.
#plot(AAPLseriesforecasts_HW)

#Making forecasts for future times not included in the original time series
#AAPLforecasts <- forecast:::forecast.HoltWinters(AAPLseriesforecasts_HW, h=252) #the value of 252 is used for h as this is the number of trading days per annum
#AAPLforecasts
#plot(AAPLforecasts)

#Checking if improvement can be made
#acf(AAPLforecasts$residuals, lag.max=20, na.action = na.pass)
#The correlogram shows that the autocorrelations for the in-sample forecast errors exceed the significance bounds for lags 1-20 at 0.01 & 0.04 - 0.06
#Box.test(AAPLforecasts$residuals, lag=20, type="Ljung-Box")
#The p-value here is very low, indicating that there is little evidence of non-zero autocorrelations at lags 1-20.

#plot.ts(AAPLforecasts$residuals)            #make a time plot
#From the time plot, it appears plausible that the forecast errors have constant variance over time

#for (i in 1:ncol(AAPLforecasts$residuals)){
 # AAPLforecasts$residuals[is.na(AAPLforecasts$residuals[,i]), i] <- mean(AAPLforecasts$residuals[,i], na.rm = TRUE)
#}
#plotForecastErrors(AAPLforecasts$residuals) #make a histogram
#From the histogram, it seems plausible that the forecast errors are normally distributed with mean zero


#ARIMA MODEL#

#Make timeseries stationary

pricediff1 <- diff(price, differences=2)
pricediff1

plot.ts(pricediff1)

#Selecting appropriate ARIMA model
#Obtaining auto correlation values
acf(pricediff1, lag.max=20, na.action = na.pass) # plot a correlogram
acf(pricediff1, lag.max=20, plot=FALSE, na.action = na.pass) # get the autocorrelation values
#autocorrelations at lags 1 (-0.450), 7 (0.148) & 8 (-0.131) exceeds the significance bounds

#Obtaining partial correlation values
pacf(pricediff1, lag.max=20, na.action = na.pass) # plot a partial correlogram
pacf(pricediff1, lag.max=20, plot=FALSE, na.action = na.pass) # get the partial autocorrelation values

#Model = ARMA(0, 2, 1)

#Forecasting using an ARIMA model
pricetimeseriesarima <- arima(price[1:600], order=c(0,2,1)) # fit an ARIMA(0,2,1) model
pricetimeseriesarima

#Making predictions
pricetforecastsarima <- forecast:::forecast.Arima(pricetimeseriesarima, h=252)
pricetforecastsarima

plot(pricetforecastsarima)

acf(pricetforecastsarima$residuals, lag.max=20, na.action = na.pass)
Box.test(pricetforecastsarima$residuals, lag=20, type="Ljung-Box")

plot.ts(pricetforecastsarima$residuals) # make time plot of forecast errors
#Starting at 3 as frrst 2 values were null and the for-loop above wasnt working
plotForecastErrors(pricetforecastsarima$residuals[3:600]) # make a histogram

#########################################################################################

                        #GRAMEVOL WITH DIFFERENT LAG#

#########################################################################################

# Define a window size - let's say 5
new_price_5 <- data.frame(x5=Lag(price,5), x4=Lag(price,4), x3=Lag(price,3), x2=Lag(price,2),
                        x1=Lag(price,1), price)
new_price_5
names(new_price_5) <- c('x5','x4','x3','x2','x1','x')
head(new_price_5)

#Create training & test data
train_5 <- new_price_5[6: 600, ]
train_5
test_5 <- new_price_5[601:754, ]
test_5

#define our grammar
newRules <- list(expr = grule(op(expr, expr), func(expr), var),
                 func = grule(sin, cos, exp),
                 op = grule('+', '-', '*', '/', '^'),
                 var = grule(lagdata$x5,lagdata$x4,lagdata$x3, lagdata$x2, lagdata$x1))

# Then need to create grammar from rules
newGram <- CreateGrammar(newRules)
newGram

# Fitness function (RMSE)
newFitFunc <- function(expr) {
  result <- eval(expr)
  if (any(is.nan(result)))
    return(Inf)
  return (sqrt(mean((lagdata$x - result)^2)))
  #return(mean(log(1 + abs(mydata$x - result))))
}

#Now use the training data to evolve a function to predict values
lagdata <- train_5

ge <- GrammaticalEvolution(newGram, newFitFunc, terminationCost = 0.01, 
                           iterations = 2500, max.depth = 5)
ge

# Get the result out
ge$best$expressions

# Look at the values generated by training
eval(ge$best$expressions)
best_train_5 <- eval(ge$best$expressions)
best_train_5
eval_train_5 <- data.frame(lagdata[1:5, 1], best_train_5[1:5])
eval_train_5

#ACCURACY SCORE#
accuracy(lagdata[1:5, 1], best_train_5[1:5])

#Then look at the performance over the test data
lagdata <- test_5
eval(ge$best$expressions)
best_test_5 <- eval(ge$best$expressions)
best_test_5
eval_test_5 <- data.frame(lagdata[1:5, 1], best_test_5[1:5])
eval_test_5

#ACCURACY SCORE#
accuracy(lagdata[1:5, 1], best_test_5[1:5])

## Predicting more than one step ahead ##
# Now use this to make further predictions - i.e. use generated values rather than exiting ones

# grab the last 5 values from the training data
lagdata <- train_5[595, 1:5]
lagdata

# initiaise an array of predictions
predictions_5 <-c()

# let's try and predict 5 values ahead, each time feeding in 
# the predicted value to use as input for new predictions

for(i in 1:5){
  predictions_5[i] <- eval(ge$best$expressions)
  # shufle data along and insert new prediction
  # put this in a loop for a large window size
  lagdata[1] <- lagdata[2] 
  lagdata[2] <- lagdata[3]
  lagdata[3] <- lagdata[4]
  lagdata[4] <- lagdata[5]
  lagdata[5] <- predictions_5[i]}

# See what the predictions are
predictions_5

test_results_5 <- data.frame(test_5[1:5, 1], predictions_5)
test_results_5

#ACCURACY SCORE#
accuracy(test_5[1:5, 1], predictions_5)

#We can see that we have got a better accuracy (RMSE score) by increasing the lag, compared to our initial model that had a lag of 3.

#################################################################################

                                #COMPARISONS#

#################################################################################

df <- data.frame(AAPLseriesforecasts2) #Forecasts from Holts Exponential Smoothing
df <- df[, 1]
head(df)
HESF <- ts(df, frequency = 120, start=c(2016-01-01))
HESF

#Holts Exponential Smoothing Forecasts vs Test Data
HESF_comparison_results <- data.frame(HESF[1:5], HES_test[1:5]) #Comparing first 5 results
HESF_comparison_results

#ACCURACY SCORE#
accuracy(HESF[1:5], HES_test[1:5])

#We can observe that the accuracy score is much worse compared to both our GramEvol models.
#Indicating that perhaps Holt Winters may be a better method to try next time. 


#ARIMA Forecasts vs Actual
price_test <- price[601: 754, ]
df_ARIMA <- data.frame(pricetforecastsarima) #Forecasts from ARIMA model
df_ARIMA <- df_ARIMA[,1]
head(df_ARIMA)
ARIMA_Forecasts <- ts(df_ARIMA, frequency = 120, start=c(2016-01-01))
ARIMA_Forecasts
ARIMA_comparison_results <- data.frame(ARIMA_Forecasts[1:5], price_test[1:5, 1])
ARIMA_comparison_results

#ACCURACY SCORE#
accuracy(ARIMA_Forecasts[1:5], price_test[1:5, 1])

#The accuracy score achieved here if far better than the 3 other forecast methods used previously.
#This could provide as a good target for out GramEvol model, as we could try increasing the lag to get an RMSE score similar to thr ARIMA model score

#Comparison between all forecasts vs actual

results <- data.frame(predictions, test[1:5, 1], predictions_5, test_5[1:5, 1], HESF[1:5], HES_test[1:5], ARIMA_Forecasts[1:5], price_test[1:5, 1])
results #Showing all the model predictions/forecasts beside their respective test(actual) values

#Accuracy scores of each method (Looking at the RMSE values)
accuracy(test[1:5, 1], predictions) #Lag of 3 
#RMSE = 1.203663
accuracy(test_5[1:5, 1], predictions_5) #Lag of 5
#RMSE = 1.069024
accuracy(HESF[1:5], HES_test[1:5]) #Holt Exponential forecast accuracy
#RMSE = 2.717842
accuracy(ARIMA_Forecasts[1:5], price_test[1:5, 1]) #ARIMA forecast accuracy
#RMSE = 0.4294146

#Evaluating the accuracy based off the RMSE scores, the ARIMA model, using the original data set has the best accuracy.
#It can also be noted that with a higher lag of 5, the GramEvol performed better than the previous model, which had a lag of 3. 
#Indicating that perhaps if an even larger lag was attemped, it may continue to improve in accuracy.
